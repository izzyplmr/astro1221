import os
import litellm
import base64
import json
import warnings
from dotenv import load_dotenv
import numpy as np
import matplotlib.pyplot as plt

# Load environment variables
load_dotenv()

# Suppress specific Pydantic warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

custom_api_base = "https://litellmproxy.osu-ai.org" 
astro1221_key = os.getenv("ASTRO1221_API_KEY")

def prompt_llm(messages, model="openai/GPT-4.1-mini", temperature=0.2, max_tokens=1000, tools=None, verbose=True):
    """
    Send a prompt or conversation to an LLM using LiteLLM and return the response.
    Exact function from Class 9 notebook.
    """
    if isinstance(messages, str):
        messages = [{"role": "user", "content": messages}]
    if not (isinstance(temperature, (int, float)) and 0 <= temperature <= 2):
        raise ValueError("temperature must be a float between 0 and 2 (inclusive).")
    if not (isinstance(max_tokens, int) and max_tokens > 0):
        raise ValueError("max_tokens must be a positive integer.")

    try: 
        print("Contacting LLM via University Server...")
        response = litellm.completion(
            model=model,
            messages=messages,
            tools=tools,
            api_base=custom_api_base,
            api_key=astro1221_key,
            temperature=temperature,
            max_tokens=max_tokens
        )
        answer = response['choices'][0]['message']['content']
        if verbose: 
            print(f"\nSUCCESS! Here is the answer from {model}:\n")
            print(answer)
    except Exception as e:
        print(f"\nERROR: Could not connect. Details:\n{e}")    
        response = None
    return response



